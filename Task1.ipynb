{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90d0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b81891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('task1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdbff30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>Same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senegal has partnered with a UK-based health d...</td>\n",
       "      <td>I'm relieved that Harvey Weinstein will finall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the man on the phone: what's it like making hi...</td>\n",
       "      <td>flame towers: luxury atop one of the world's t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>, Callum Michael RebelJenna Dewan has announce...</td>\n",
       "      <td>The current crop of twentysomethings are going...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>learning to live with the coronavirus q if i h...</td>\n",
       "      <td>guinea-bissau: political chaos could boost coc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>athletes allege abuse, racism at u. of illinoi...</td>\n",
       "      <td>contact tracing is key to america's coronaviru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  T1  \\\n",
       "0  Senegal has partnered with a UK-based health d...   \n",
       "1  the man on the phone: what's it like making hi...   \n",
       "2  , Callum Michael RebelJenna Dewan has announce...   \n",
       "3  learning to live with the coronavirus q if i h...   \n",
       "4  athletes allege abuse, racism at u. of illinoi...   \n",
       "\n",
       "                                                  T2  Same  \n",
       "0  I'm relieved that Harvey Weinstein will finall...     1  \n",
       "1  flame towers: luxury atop one of the world's t...     1  \n",
       "2  The current crop of twentysomethings are going...     1  \n",
       "3  guinea-bissau: political chaos could boost coc...     1  \n",
       "4  contact tracing is key to america's coronaviru...     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f81e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['T1','T2','Same']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6481c1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>Same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senegal has partnered with a UK-based health d...</td>\n",
       "      <td>I'm relieved that Harvey Weinstein will finall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the man on the phone: what's it like making hi...</td>\n",
       "      <td>flame towers: luxury atop one of the world's t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>, Callum Michael RebelJenna Dewan has announce...</td>\n",
       "      <td>The current crop of twentysomethings are going...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>learning to live with the coronavirus q if i h...</td>\n",
       "      <td>guinea-bissau: political chaos could boost coc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>athletes allege abuse, racism at u. of illinoi...</td>\n",
       "      <td>contact tracing is key to america's coronaviru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  T1  \\\n",
       "0  Senegal has partnered with a UK-based health d...   \n",
       "1  the man on the phone: what's it like making hi...   \n",
       "2  , Callum Michael RebelJenna Dewan has announce...   \n",
       "3  learning to live with the coronavirus q if i h...   \n",
       "4  athletes allege abuse, racism at u. of illinoi...   \n",
       "\n",
       "                                                  T2  Same  \n",
       "0  I'm relieved that Harvey Weinstein will finall...     1  \n",
       "1  flame towers: luxury atop one of the world's t...     1  \n",
       "2  The current crop of twentysomethings are going...     1  \n",
       "3  guinea-bissau: political chaos could boost coc...     1  \n",
       "4  contact tracing is key to america's coronaviru...     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4223a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79298488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\girid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\girid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\girid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\girid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Here\n",
      "Here\n",
      "Here\n",
      "Here\n",
      "Here\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# load the data into a pandas DataFrame\n",
    "df = pd.read_csv('task1.csv')\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['T1', 'T2']], df['Same'], test_size=0.2, random_state=42)\n",
    "\n",
    "# preprocess the text data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    # remove special characters and punctuations\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text, re.I|re.A)\n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"Here\")\n",
    "X_train['T1'] = X_train['T1'].apply(preprocess)\n",
    "\n",
    "print(\"Here\")\n",
    "X_train['T2'] = X_train['T2'].apply(preprocess)\n",
    "\n",
    "print(\"Here\")\n",
    "X_test['T1'] = X_test['T1'].apply(preprocess)\n",
    "\n",
    "print(\"Here\")\n",
    "X_test['T2'] = X_test['T2'].apply(preprocess)\n",
    "\n",
    "print(\"Here\")\n",
    "# convert text data into TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['T1'] + ' ' + X_train['T2'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['T1'] + ' ' + X_test['T2'])\n",
    "\n",
    "print(\"Here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a40bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Here\n",
      "Accuracy: 0.521\n",
      "Precision: 0.5211500696100239\n",
      "Recall: 0.521\n",
      "F1-score: 0.521\n"
     ]
    }
   ],
   "source": [
    "# train a SVM classifier on the training data\n",
    "print(\"Here\")\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Here\")\n",
    "# predict the target variable for the testing data\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "# evaluate the performance of the classifier\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred,average='weighted'))\n",
    "print('Recall:', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e3e580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93b14292",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=1234,n_estimators=150,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c533f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c34d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.726\n",
      "Precision: 0.7270752882205513\n",
      "Recall: 0.726\n",
      "F1-score: 0.7258385557186144\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred,average='weighted'))\n",
    "print('Recall:', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "334bf472",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "fit = clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ebd1139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5155\n",
      "Precision: 0.5156160671872921\n",
      "Recall: 0.5155\n",
      "F1-score: 0.5155115069469184\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred,average='weighted'))\n",
    "print('Recall:', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efd574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1332d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl = pd.read_csv('data\\\\new_ctrl.csv')\n",
    "gpt = pd.read_csv('data\\\\new_gpt.csv')\n",
    "gpt2 = pd.read_csv('data\\\\new_gpt2.csv')\n",
    "\n",
    "grover = pd.read_csv('data\\\\new_grover.csv')\n",
    "xlm = pd.read_csv('data\\\\new_xlm.csv')\n",
    "xlnet = pd.read_csv('data\\\\new_xlnet.csv')\n",
    "\n",
    "pplm = pd.read_csv('data\\\\new_pplm.csv')\n",
    "human = pd.read_csv('data\\\\new_human.csv')\n",
    "fair = pd.read_csv('data\\\\new_fair.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7af36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_binary_t2 = pd.concat([human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), grover.sample(1066), \n",
    "                              xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), ctrl.sample(1066), \n",
    "                              gpt.sample(1066), gpt2.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), \n",
    "                              pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), human.sample(1066), \n",
    "                              gpt2.sample(1066), gpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), \n",
    "                              pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066), \n",
    "                              gpt2.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), \n",
    "                              fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), \n",
    "                              grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), \n",
    "                              human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), grover.sample(1066), \n",
    "                              xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), human.sample(1066), \n",
    "                              ctrl.sample(1066), gpt.sample(1066), gpt2, grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), \n",
    "                              pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), \n",
    "                          gpt.sample(1066), gpt2.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), \n",
    "                              fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), \n",
    "                              grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c163ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_binary_t1 = pd.concat([human,human, human, ctrl, ctrl, ctrl, gpt, gpt, gpt, gpt2, gpt2, gpt2, grover, grover, grover, xlm, xlm, xlm, xlnet, xlnet, xlnet, \n",
    "                             pplm, pplm, pplm, fair, fair, fair])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3277bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = [np.ones(len(human)), \n",
    "             np.zeros(8 * 1066), \n",
    "             np.ones(len(ctrl)), \n",
    "             np.zeros(11 * 1066), \n",
    "             np.ones(len(gpt)), \n",
    "             np.zeros(8 * 1066),\n",
    "             np.ones(len(gpt2)),\n",
    "             np.zeros(9 * 1066),\n",
    "             np.ones(len(grover)),\n",
    "             np.zeros(9 * 1066),\n",
    "             np.ones(len(xlm)),\n",
    "             np.zeros(9 * 1066),\n",
    "             np.ones(len(xlnet)),\n",
    "             np.zeros(9 * 1066),\n",
    "             np.ones(len(pplm)),\n",
    "             np.zeros(9 * 1066),\n",
    "             np.ones(len(fair))\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53b30490",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in new_label for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f537bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_random = pd.DataFrame({'T1': list(random_binary_t1['Generation']), 'T2': list(random_binary_t2['Generation']), 'class': flat_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe35e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_random.to_csv('data\\\\binary_random.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\girid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\girid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\girid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\girid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# load the data into a pandas DataFrame\n",
    "df = pd.read_csv('data\\\\binary_random.csv')\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['T1', 'T2']], df['class'], test_size=0.2, random_state=42)\n",
    "\n",
    "# preprocess the text data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # remove special characters and punctuations\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text, re.I|re.A)\n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "X_train['T1'] = X_train['T1'].apply(preprocess)\n",
    "X_train['T2'] = X_train['T2'].apply(preprocess)\n",
    "X_test['T1'] = X_test['T1'].apply(preprocess)\n",
    "X_test['T2'] = X_test['T2'].apply(preprocess)\n",
    "\n",
    "# convert text data into TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['T1'] + ' ' + X_train['T2'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['T1'] + ' ' + X_test['T2'])\n",
    "\n",
    "# train a SVM classifier on the training data\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# predict the target variable for the testing data\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "# evaluate the performance of the classifier\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred,average='weighted'))\n",
    "print('Recall:', recall_score(y_test, y_pred, average='weighted'))\n",
    "print('F1-score:', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd6017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
